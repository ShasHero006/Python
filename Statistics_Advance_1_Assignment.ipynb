{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMXKAqCzEc3s38nh1Zji58",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShasHero006/Python/blob/main/Statistics_Advance_1_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q.1. Explain the properties of the F-distribution.**\n",
        "\n",
        "Ans. The F-distribution, also known as Snedecor's F-distribution or the Fisher-Snedecor distribution, is a continuous probability distribution that arises frequently in statistical hypothesis testing.  Here are its key properties:\n",
        "\n",
        " 1. **Non-negative Values:** The F-distribution is defined only for non-negative values.  The random variable F can take on values from 0 to positive infinity. It cannot be negative.\n",
        "\n",
        " 2. **Skewness to the Right (Positive Skew):** The F-distribution is skewed to the right. This means its tail extends further to the right than to the left.  The degree of skewness depends on the degrees of freedom of the numerator and denominator. As the degrees of freedom increase, the distribution becomes more symmetric.\n",
        "\n",
        " 3. **Defined by Two Degrees of Freedom:** Unlike many other distributions, the F-distribution is characterized by *two* degrees of freedom:  \n",
        "    * **Numerator Degrees of Freedom (df1):**  Relates to the variability in the numerator of the F-statistic (often related to the variance of a sample or a group).\n",
        "     * **Denominator Degrees of Freedom (df2):** Relates to the variability in the denominator of the F-statistic (often related to the variance of another sample or group, or a population variance estimate).\n",
        "     \n",
        " 4. **Shape Dependence on Degrees of Freedom:** The shape of the F-distribution is heavily influenced by the values of df1 and df2.  As df1 and df2 increase, the distribution becomes more symmetric and approaches a normal distribution.\n",
        "\n",
        " 5. **Relationship to Chi-Square Distribution:**  The F-distribution is closely related to the chi-square distribution. The F-statistic is the ratio of two independent chi-square random variables, each divided by its respective degrees of freedom.\n",
        "\n",
        " 6. **Use in ANOVA and Regression:** The F-distribution is primarily used in analysis of variance (ANOVA) and regression analysis. It's the basis for the F-test, a statistical test used to compare variances between groups or to assess the overall significance of a regression model.\n",
        "\n",
        " 7. **Mean and Variance:**  The mean and variance of an F-distribution depend on the degrees of freedom.  For df2 > 2, the mean is df2 / (df2 - 2).  For df2 > 4, the variance is (2 * df2^2 * (df1 + df2 - 2)) / (df1 * (df2 - 2)^2 * (df2 - 4)).\n",
        "\n",
        " 8. **Asymptotic Behavior:** As the degrees of freedom (both df1 and df2) approach infinity, the F-distribution approaches a normal distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " __________________________________________________________________________"
      ],
      "metadata": {
        "id": "-qlxg07pi_kS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q.2.  In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**\n",
        "\n",
        "Ans. The F-distribution is primarily used in statistical tests where we compare variances or assess the overall significance of models that involve variances.  Here are the main types of tests:\n",
        "\n",
        "1. **Analysis of Variance (ANOVA):**  ANOVA uses the F-test to determine if there are statistically significant differences among the means of two or more groups. The F-statistic is calculated as the ratio of the variance *between* the groups to the variance *within* the groups.  A larger F-statistic suggests that the variance between groups is much larger than the variance within groups, providing evidence that the group means are different.  The F-distribution provides the probability distribution for this ratio under the null hypothesis (that all group means are equal), enabling us to calculate a p-value and determine statistical significance.\n",
        "\n",
        "2. **Regression Analysis:** In regression analysis, the F-test assesses the overall significance of the regression model. It tests whether *any* of the independent variables are significantly related to the dependent variable.  The F-statistic is the ratio of the explained variance (variance in the dependent variable explained by the model) to the unexplained variance (residual variance). A large F-statistic indicates that the model is explaining a significant amount of variance in the dependent variable.  The F-distribution helps determine if this explanation is statistically significant.\n",
        "\n",
        "3. **Comparing Two Variances:** The F-test can be used to directly compare the variances of two populations.  It assumes that the samples are drawn from normal distributions.  The F-statistic is simply the ratio of the two sample variances. The F-distribution allows us to determine the probability of observing such a large ratio if the population variances are actually equal.\n",
        "\n",
        "**Why is the F-distribution appropriate?**\n",
        "\n",
        "The F-distribution is appropriate for these tests because it's derived from the ratio of two independent chi-square random variables, each divided by its respective degrees of freedom.  This ratio naturally arises when comparing variances, as variance estimates are often related to chi-square distributions.  The F-distribution provides a framework for determining the probability of observing a given ratio under the null hypothesis, allowing for statistically sound conclusions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "___________________________________________________________________________"
      ],
      "metadata": {
        "id": "_oQWDpKKlUgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q.3.  What are the key assumptions required for conducting an F-test to compare the variances of two populations?**\n",
        "\n",
        "Ans. The F-test to compare the variances of two populations relies on several key assumptions to ensure the validity of the results. These assumptions are as follows:\n",
        "\n",
        "1. **Normality**\n",
        "The populations from which the samples are drawn must follow a normal distribution.\n",
        "The F-test is highly sensitive to departures from normality. If this assumption is violated, the test may lead to incorrect conclusions.\n",
        "2. **Independence**\n",
        "The samples must be independent of each other.\n",
        "This means the observations in one sample should not influence the observations in the other sample.\n",
        "3. **Random Sampling**\n",
        "The data should be collected through random sampling or random assignment to ensure that the samples are representative of their respective populations.\n",
        "4. **Measurement Scale**\n",
        "The data should be measured on at least an interval scale (or higher). This ensures the meaningful calculation of variances.\n",
        "5. **Finite Population Variance**\n",
        "Both populations must have finite variances, as the test is designed to compare these variances.\n",
        "\n",
        "Considerations When Assumptions Are Violated :-    \n",
        "If the normality assumption is violated, consider using a non-parametric test like the Levene's test or Brown-Forsythe test, which are more robust to deviations from normality.\n",
        "\n",
        "If the sample sizes are large, the F-test is relatively robust to minor deviations from normality (due to the Central Limit Theorem).\n",
        "For dependence between samples, alternative paired-sample methods should be used.\n",
        "\n",
        "By adhering to these assumptions, the F-test provides reliable results for comparing variances between two populations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "___________________________________________________________________________\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G7DdSGiLmLnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q.4. What is the purpose of ANOVA, and how does it differ from a t-test?**\n",
        "\n",
        "Ans. Purpose of ANOVA\n",
        "ANOVA (Analysis of Variance) is a statistical technique used to compare the means of three or more groups to determine if there are significant differences among them. Instead of testing each pair of means individually, ANOVA evaluates all group means simultaneously by analyzing the variability within and between groups.\n",
        "\n",
        "Key Goal: To determine whether the variability between group means is greater than the variability within the groups.\n",
        "\n",
        "Applications: Commonly used in experiments or studies with multiple groups or conditions, such as comparing treatment effects or analyzing data from different demographic groups.\n",
        "\n",
        "\n",
        "\n",
        "Difference Between ANOVA and a t-Test :-   \n",
        "The core difference lies in the number of groups being compared:\n",
        "\n",
        " t-test: Used to compare the means of *two* groups.  It can be a one-sample t-test (comparing a sample mean to a known population mean), an independent samples t-test (comparing the means of two independent groups), or a paired samples t-test (comparing the means of two related groups).\n",
        "\n",
        " ANOVA: Used to compare the means of *three or more* groups.  It tests the null hypothesis that all group means are equal.  If the null hypothesis is rejected, it indicates that at least one group mean is significantly different from the others, but it doesn't specify which groups differ.  Further post-hoc tests (like Tukey's HSD or Bonferroni correction) are needed to identify the specific differences between groups.\n",
        "\n",
        "\n",
        " In summary:\n",
        "\n",
        " Use a t-test when comparing two groups.\n",
        " Use ANOVA when comparing three or more groups.\n",
        "\n",
        " Key Relationship Between ANOVA and t-Test :-  \n",
        "A two-sample t-test is a special case of ANOVA for two groups.\n",
        "The F-statistic in ANOVA is related to the t-statistic:\n",
        "`F= t**2`\n",
        "  for a two-group comparison.\n",
        "\n",
        "Practical Example :    \n",
        "t-Test Example: Comparing the average test scores of students in two different classes (Class A vs. Class B).\n",
        "\n",
        "ANOVA Example: Comparing the average test scores of students in three or more classes (Class A, Class B, and Class C).\n",
        "\n",
        "ANOVA is more versatile for analyzing data with multiple groups and helps avoid the error inflation that occurs when conducting multiple t-tests on the same data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "___________________________________________________________________________\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7P0M_sognbO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q.5.  Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.**\n",
        "\n",
        "\n",
        "Ans. One-Way ANOVA vs. Multiple T-Tests :     \n",
        "\n",
        "When comparing means across more than two groups, One-Way ANOVA (Analysis of Variance) is preferred over conducting multiple t-tests for the following reasons:\n",
        "\n",
        "1. Controlling Type I Error Problem with Multiple T-Tests: Each t-test carries a risk of a Type I error (rejecting a true null hypothesis). If you perform multiple t-tests, these errors accumulate. For example, with a significance level (\n",
        "ùõº ) of 0.05, performing three t-tests increases the probability of making at least one Type I error:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "At¬†least¬†one¬†Type¬†I¬†error\n",
        ")\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõº\n",
        ")\n",
        "ùëò\n",
        "P(At¬†least¬†one¬†Type¬†I¬†error)=1‚àí(1‚àíŒ±)\n",
        "k\n",
        "\n",
        "where\n",
        "ùëò\n",
        "k is the number of tests. For\n",
        "ùëò\n",
        "=\n",
        "3\n",
        "k=3, the risk of error rises to about 14% instead of the intended 5%.\n",
        "\n",
        "Solution with ANOVA: One-Way ANOVA tests the null hypothesis that all group means are equal with a single test, maintaining the overall\n",
        "ùõº\n",
        "Œ± level at 0.05.\n",
        "\n",
        "2. Efficiency\n",
        "\n",
        "Conducting a single ANOVA test is more efficient and requires fewer calculations compared to performing multiple pairwise t-tests, especially as the number of groups increases.\n",
        "3. Hypothesis Testing Framework ANOVA: Tests whether there are any statistically significant differences among the group means. It does not test each pair but evaluates the overall group variation:\n",
        "\n",
        "Null Hypothesis (\n",
        "ùêª\n",
        "0\n",
        "H\n",
        "0\n",
        "‚Äã\n",
        " ): All group means are equal (\n",
        "ùúá\n",
        "1\n",
        "=\n",
        "ùúá\n",
        "2\n",
        "=\n",
        "ùúá\n",
        "3\n",
        "=\n",
        "‚Ä¶\n",
        "Œº\n",
        "1\n",
        "‚Äã\n",
        " =Œº\n",
        "2\n",
        "‚Äã\n",
        " =Œº\n",
        "3\n",
        "‚Äã\n",
        " =‚Ä¶).\n",
        "Alternative Hypothesis (\n",
        "ùêª\n",
        "ùê¥\n",
        "H\n",
        "A\n",
        "‚Äã\n",
        " ): At least one group mean differs.\n",
        "Multiple t-tests: Test pairwise differences (\n",
        "ùúá\n",
        "1\n",
        "‚â†\n",
        "ùúá\n",
        "2\n",
        "Œº\n",
        "1\n",
        "‚Äã\n",
        "\n",
        "ÓÄ†\n",
        "=Œº\n",
        "2\n",
        "‚Äã\n",
        " ,\n",
        "ùúá\n",
        "1\n",
        "‚â†\n",
        "ùúá\n",
        "3\n",
        "Œº\n",
        "1\n",
        "‚Äã\n",
        "\n",
        "ÓÄ†\n",
        "=Œº\n",
        "3\n",
        "‚Äã\n",
        " , etc.) without considering the broader pattern of variation across all groups.\n",
        "\n",
        "4. ANOVA Post-Hoc Analysis\n",
        "\n",
        "If One-Way ANOVA finds significant differences, post-hoc tests (e.g., Tukey's HSD) can be used to identify which specific groups differ while adjusting for multiple comparisons. This approach balances error control with identifying differences.\n",
        "\n",
        "Example Scenario:\n",
        "\n",
        "Suppose you are comparing the mean test scores of students across three teaching methods: A, B, and C.\n",
        "\n",
        "Conducting multiple t-tests would compare:\n",
        "\n",
        "Method A vs. Method B\n",
        "Method A vs. Method C\n",
        "Method B vs. Method C\n",
        "\n",
        "This increases the risk of Type I errors.\n",
        "\n",
        "Instead, use One-Way ANOVA:\n",
        "\n",
        "Tests whether the mean scores for the three methods are significantly different in one test.\n",
        "\n",
        "If significant, follow with post-hoc tests to identify which methods differ.\n",
        "\n",
        "When to Use One-Way ANOVA:\n",
        "\n",
        "When comparing means across three or more groups.\n",
        "\n",
        "When you want to avoid increasing the probability of Type I errors.\n",
        "\n",
        "When the assumptions of ANOVA (normality, homogeneity of variances) are met or reasonably approximated.\n",
        "\n",
        "In summary, One-Way ANOVA is a robust, error-controlled, and efficient method for comparing more than two group means, making it the preferred choice over multiple t-tests in most cases.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "___________________________________________________________________________\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mCeOYfUvqpLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q.6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?**\n",
        "\n",
        "Ans. Variance Partitioning in ANOVA\n",
        "\n",
        "In One-Way ANOVA, the total variation in the data is partitioned into two components:\n",
        "\n",
        "Between-Group Variance (explained variance): Measures the variation due to differences in group means.\n",
        "\n",
        "Within-Group Variance (unexplained variance): Measures the variation within each group, attributed to individual differences and random error.\n",
        "\n",
        "The sum of these two components equals the Total Variance:\n",
        "\n",
        "Total¬†Variance\n",
        "=\n",
        "Between-Group¬†Variance\n",
        "+\n",
        "Within-Group¬†Variance\n",
        "Total¬†Variance=Between-Group¬†Variance+Within-Group¬†Variance\n",
        "\n",
        "Key Components in ANOVA\n",
        "1. Total Sum of Squares (SST)\n",
        "Represents the total variability in the data around the grand mean (\n",
        "ùëã\n",
        "Àâ\n",
        "X\n",
        "Àâ\n",
        " ).\n",
        "\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùëá\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùëñ\n",
        "(\n",
        "ùëã\n",
        "ùëñ\n",
        "ùëó\n",
        "‚àí\n",
        "ùëã\n",
        "Àâ\n",
        ")\n",
        "2\n",
        "SST=\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "‚Äã\n",
        "  \n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        " (X\n",
        "ij\n",
        "‚Äã\n",
        " ‚àí\n",
        "X\n",
        "Àâ\n",
        " )\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëã\n",
        "ùëñ\n",
        "ùëó\n",
        "X\n",
        "ij\n",
        "‚Äã\n",
        "  is the\n",
        "ùëó\n",
        "j-th observation in the\n",
        "ùëñ\n",
        "i-th group.\n",
        "ùëã\n",
        "Àâ\n",
        "X\n",
        "Àâ\n",
        "  is the grand mean (mean of all observations).\n",
        "\n",
        "2. Between-Group Sum of Squares (SSB)\n",
        "\n",
        "Measures the variation due to differences between the group means (\n",
        "ùëã\n",
        "Àâ\n",
        "ùëñ\n",
        "X\n",
        "Àâ\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        " ) and the grand mean (\n",
        "ùëã\n",
        "Àâ\n",
        "X\n",
        "Àâ\n",
        " ).\n",
        "\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "ùëõ\n",
        "ùëñ\n",
        "(\n",
        "ùëã\n",
        "Àâ\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëã\n",
        "Àâ\n",
        ")\n",
        "2\n",
        "SSB=\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "‚Äã\n",
        " n\n",
        "i\n",
        "‚Äã\n",
        " (\n",
        "X\n",
        "Àâ\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "X\n",
        "Àâ\n",
        " )\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëò\n",
        "k is the number of groups.\n",
        "ùëõ\n",
        "ùëñ\n",
        "n\n",
        "i\n",
        "‚Äã\n",
        "  is the number of observations in the\n",
        "ùëñ\n",
        "i-th group.\n",
        "ùëã\n",
        "Àâ\n",
        "ùëñ\n",
        "X\n",
        "Àâ\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        "  is the mean of the\n",
        "ùëñ\n",
        "i-th group.\n",
        "\n",
        "3. Within-Group Sum of Squares (SSW)\n",
        "\n",
        "Measures the variation within each group (how individual observations deviate from their group mean).\n",
        "\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùëä\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùëñ\n",
        "(\n",
        "ùëã\n",
        "ùëñ\n",
        "ùëó\n",
        "‚àí\n",
        "ùëã\n",
        "Àâ\n",
        "ùëñ\n",
        ")\n",
        "2\n",
        "SSW=\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "‚Äã\n",
        "  \n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        " (X\n",
        "ij\n",
        "‚Äã\n",
        " ‚àí\n",
        "X\n",
        "Àâ\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "Partitioning Relationship\n",
        "\n",
        "The total variability is partitioned as:\n",
        "\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùëá\n",
        "=\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "+\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùëä\n",
        "SST=SSB+SSW\n",
        "\n",
        "This relationship is fundamental to ANOVA and forms the basis for calculating the F-statistic.\n",
        "\n",
        "Contribution to the F-Statistic\n",
        "\n",
        "The F-statistic in ANOVA compares the variability explained by the group differences (between-group variance) to the variability within groups (within-group variance). It is calculated as:\n",
        "\n",
        "ùêπ\n",
        "=\n",
        "Mean¬†Square¬†Between¬†(MSB)\n",
        "Mean¬†Square¬†Within¬†(MSW)\n",
        "F=\n",
        "Mean¬†Square¬†Within¬†(MSW)\n",
        "Mean¬†Square¬†Between¬†(MSB)\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "Mean Square Between (MSB): Average variance due to group differences.\n",
        "\n",
        "ùëÄ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "=\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "ùëò\n",
        "‚àí\n",
        "1\n",
        "MSB=\n",
        "k‚àí1\n",
        "SSB\n",
        "‚Äã\n",
        "\n",
        "(\n",
        "ùëò\n",
        "‚àí\n",
        "1\n",
        "k‚àí1 is the degrees of freedom for the between-group component).\n",
        "\n",
        "Mean Square Within (MSW): Average variance within groups.\n",
        "\n",
        "ùëÄ\n",
        "ùëÜ\n",
        "ùëä\n",
        "=\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùëä\n",
        "ùëÅ\n",
        "‚àí\n",
        "ùëò\n",
        "MSW=\n",
        "N‚àík\n",
        "SSW\n",
        "‚Äã\n",
        "\n",
        "(\n",
        "ùëÅ\n",
        "‚àí\n",
        "ùëò\n",
        "N‚àík is the degrees of freedom for the within-group component, where\n",
        "ùëÅ is the total number of observations).\n",
        "\n",
        "Interpreting the F-Statistic\n",
        "\n",
        "If\n",
        "ùêπ\n",
        ">\n",
        "1\n",
        "F>1, it suggests that the between-group variance is larger than the within-group variance, indicating potential differences among the group means.\n",
        "\n",
        "A large F-statistic (relative to the critical value) leads to rejecting the null hypothesis, concluding that at least one group mean is significantly different.\n",
        "\n",
        "Example:\n",
        "\n",
        "Data:\n",
        "\n",
        "Three groups of scores:\n",
        "\n",
        "ùê¥\n",
        "=\n",
        "[\n",
        "5\n",
        ",\n",
        "6\n",
        ",\n",
        "7\n",
        "]\n",
        "A=[5,6,7],\n",
        "ùêµ\n",
        "=\n",
        "[\n",
        "8\n",
        ",\n",
        "9\n",
        ",\n",
        "10\n",
        "]\n",
        "B=[8,9,10],\n",
        "ùê∂\n",
        "=\n",
        "[\n",
        "4\n",
        ",\n",
        "5\n",
        ",\n",
        "6\n",
        "]\n",
        "C=[4,5,6].\n",
        "\n",
        "Total Sum of Squares (SST): Measures how far each observation deviates from the grand mean.\n",
        "\n",
        "Between-Group Sum of Squares (SSB): Measures how far the group means deviate from the grand mean.\n",
        "\n",
        "Within-Group Sum of Squares (SSW): Measures variability within each group.\n",
        "\n",
        "Calculation of F:\n",
        "\n",
        "Compute\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "SSB,\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùëä\n",
        "SSW,\n",
        "ùëÄ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "MSB, and\n",
        "ùëÄ\n",
        "ùëÜ\n",
        "ùëä\n",
        "MSW.\n",
        "Compute\n",
        "ùêπ\n",
        "=\n",
        "ùëÄ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "/\n",
        "ùëÄ\n",
        "ùëÜ\n",
        "ùëä\n",
        "F=MSB/MSW.\n",
        "Compare\n",
        "ùêπ\n",
        "F to the critical\n",
        "ùêπ\n",
        "F-value to determine significance.\n",
        "\n",
        "This partitioning and calculation give insight into whether group means differ significantly.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "___________________________________________________________________________\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tjvw7c2nrsHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q.7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**\n",
        "\n",
        "Ans. The classical (frequentist) approach to ANOVA and the Bayesian approach to ANOVA differ significantly in how they handle uncertainty, parameter estimation, and hypothesis testing. Here‚Äôs a comparison of the two:\n",
        "\n",
        "1. Handling of Uncertainty\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "Uncertainty is based on sampling variability.\n",
        "Relies on confidence intervals and\n",
        "p-values derived from the distribution of test statistics under the null hypothesis.\n",
        "\n",
        "Assumes that the data comes from a fixed, repeatable process.\n",
        "Does not provide a direct probability of a hypothesis being true.\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "Uncertainty is quantified using probability distributions for parameters (posterior distributions).\n",
        "\n",
        "Directly provides the probability of a hypothesis or model being true given the observed data.\n",
        "\n",
        "Incorporates prior knowledge or beliefs (prior distributions) along with the observed data (likelihood).\n",
        "\n",
        "2. Parameter Estimation\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "Estimates parameters (e.g., group means, variances) based solely on the observed data.\n",
        "\n",
        "Uses point estimates like sample means and variances.\n",
        "Parameters are considered fixed but unknown quantities.\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "Estimates parameters as probability distributions (posterior distributions).\n",
        "\n",
        "Combines the likelihood of the observed data with prior distributions to produce the posterior.\n",
        "\n",
        "Accounts for both the observed data and prior beliefs about parameters.\n",
        "\n",
        "3. Hypothesis Testing\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "Tests hypotheses using p-values and test statistics (e.g., the F-statistic in ANOVA).\n",
        "\n",
        "Compares observed data against the null hypothesis to decide whether to reject it.\n",
        "\n",
        "The null hypothesis is either rejected or not rejected; no probability is assigned to the hypothesis itself.\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "Compares hypotheses or models using posterior probabilities or Bayes Factors.\n",
        "\n",
        "Provides a probability distribution for each hypothesis or model.\n",
        "\n",
        "Does not \"reject\" hypotheses outright but quantifies evidence in favor of one hypothesis over another.\n",
        "\n",
        "4. Model Assumptions\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "Assumes normality, independence, and homogeneity of variances for valid results.\n",
        "\n",
        "Model assumptions are tested separately (e.g., Shapiro-Wilk for normality, Levene‚Äôs test for equal variances).\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "Explicitly incorporates assumptions and uncertainties into the model.\n",
        "Can account for more complex data structures and relax some assumptions.\n",
        "Robust to smaller sample sizes due to the use of prior information.\n",
        "\n",
        "5. Interpretation of Results\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "Focuses on whether the observed data provides enough evidence to reject the null hypothesis.\n",
        "\n",
        "Results are typically summarized with an F-statistic, p-value, and confidence intervals.\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "Focuses on estimating the probability of hypotheses and model parameters.\n",
        "Results are presented as posterior distributions, credible intervals, and Bayes Factors.\n",
        "\n",
        "Key Strengths and Limitations\n",
        "\n",
        "Aspect\tFrequentist Approach\tBayesian Approach\n",
        "Flexibility\tLimited to pre-defined models.\tHighly flexible, can model complex data.\n",
        "\n",
        "Computational Cost\tRelatively low for standard ANOVA.\tComputationally intensive (e.g., MCMC).\n",
        "\n",
        "Prior Information\tDoes not use prior knowledge.\tIncorporates prior knowledge effectively.\n",
        "\n",
        "Small Sample Sizes\tLess reliable with small samples.\tMore robust due to priors.\n",
        "\n",
        "Interpretation\tIndirect via p-values and confidence intervals.\tDirect via probabilities and distributions.\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose we are testing if three teaching methods differ in effectiveness (measured as exam scores).\n",
        "\n",
        "Frequentist: Compute an F-statistic and p-value to decide if the mean scores are significantly different. Interpretation focuses on rejecting or not rejecting the null hypothesis.\n",
        "\n",
        "Bayesian: Assign priors to the mean scores of each group. Use observed data to update priors and compute posterior distributions for the group means. The result gives probabilities of differences between the methods and credible intervals for the mean scores.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The frequentist approach is straightforward and computationally efficient, making it suitable for routine analyses. The Bayesian approach is more flexible and informative, especially when incorporating prior knowledge or handling complex data structures, but requires more computation and expertise. The choice depends on the context, the nature of the data, and the goals of the analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "___________________________________________________________________________\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tZM6f5hRv68O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8.  Question: You have two sets of data representing the incomes of two different professions\n",
        "\n",
        " Profession A: [48, 52, 55, 60, 62]\n",
        "\n",
        " Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        " Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "  Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
        "\n",
        "  Ans."
      ],
      "metadata": {
        "id": "iVI893lW0z3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Incomes for each profession\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate variances\n",
        "var_a = np.var(profession_a, ddof=1)\n",
        "var_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "# Calculate F-statistic\n",
        "f_statistic = var_a / var_b\n",
        "\n",
        "# Calculate p-value (two-tailed test)\n",
        "dof_a = len(profession_a) - 1\n",
        "dof_b = len(profession_b) - 1\n",
        "p_value = 2 * min(stats.f.cdf(f_statistic, dof_a, dof_b), 1 - stats.f.cdf(f_statistic, dof_a, dof_b))\n",
        "\n",
        "# Output the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-mdmPe_3YaV",
        "outputId": "efcbc983-6295-43df-ce11-2584f9803fdf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.49304859900533904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__________________________________________________________________________"
      ],
      "metadata": {
        "id": "T3qWc0k14HIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9.  Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data\n",
        "\n",
        " Region A: [160, 162, 165, 158, 164]\n",
        "\n",
        " Region B: [172, 175, 170, 168, 174]\n",
        "\n",
        " Region C: [180, 182, 179, 185, 183]\n",
        "\n",
        " Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        " Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
        "\n",
        " Ans."
      ],
      "metadata": {
        "id": "bJvo0eZH3k8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Heights for each region\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Output the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mPMmsTV36N6",
        "outputId": "ce33589b-a1df-4212-8a4c-3a1da18ad5c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "___________________________________________________________________________"
      ],
      "metadata": {
        "id": "WkPIgFEV4Dwb"
      }
    }
  ]
}